# -*- coding: utf-8 -*-
"""diet-recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10JTMO-gYtChcM0nqc0TEAjfaqK9P1kvq
"""

# Import libraries
import pandas as pd
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from scipy.sparse import hstack, issparse, csr_matrix
import re
import implicit
import warnings
warnings.filterwarnings('ignore')

# load file
json_file_path = '/content/api-diets.json'

# --- 1. Load Data ---
def load_data(file_path):
    """Load and validate JSON data"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            recipes_data = json.load(f)

        if isinstance(recipes_data, list) and len(recipes_data) > 0:
            df = pd.DataFrame(recipes_data)
            print("DataFrame loaded successfully!")
            print(f"Initial DataFrame shape: {df.shape}")
            print("\nFirst 5 rows of the DataFrame:")
            print(df.head())
            return df
        else:
            print(f"Error: Expected JSON to be a non-empty list, but found {type(recipes_data)}.")
            return pd.DataFrame()

    except FileNotFoundError:
        print(f"Error: The file '{file_path}' was not found.")
        return pd.DataFrame()
    except json.JSONDecodeError as e:
        print(f"Error: Could not decode JSON from '{file_path}': {e}")
        return pd.DataFrame()
    except Exception as e:
        print(f"An unexpected error occurred during data loading: {e}")
        return pd.DataFrame()

# Load the data
df = load_data(json_file_path)

if df.empty:
    print("Cannot proceed without data. Please check your JSON file.")
    exit()

# --- 2. Data Preprocessing ---
def preprocess_data(df):
    """Comprehensive data preprocessing"""

    # Extract category name from nested dictionary
    if 'category' in df.columns:
        df['category_name'] = df['category'].apply(
            lambda x: x.get('category', 'Unknown') if isinstance(x, dict) else 'Unknown'
        )
        print("Category column processed.")

    # Combine ingredient columns
    ingredient_cols = [f'ingredient_{i}' for i in range(1, 11) if f'ingredient_{i}' in df.columns]
    if ingredient_cols:
        df['all_ingredients'] = df[ingredient_cols].fillna('').astype(str).agg(' '.join, axis=1)
        df['all_ingredients'] = df['all_ingredients'].apply(
            lambda x: ' '.join(word for word in x.split() if word.lower() not in ['none', 'nan', ''])
        ).str.strip()
        print("Ingredient columns combined.")

    # Combine directions columns
    directions_cols = [f'directions_step_{i}' for i in range(1, 11) if f'directions_step_{i}' in df.columns]
    if directions_cols:
        df['all_directions'] = df[directions_cols].fillna('').astype(str).agg(' '.join, axis=1)
        df['all_directions'] = df['all_directions'].apply(
            lambda x: ' '.join(word for word in x.split() if word.lower() not in ['none', 'nan', ''])
        ).str.strip()
        print("Directions columns combined.")

    # Handle missing numerical values
    numerical_cols = ['prep_time_in_minutes', 'cook_time_in_minutes', 'serving',
                     'fat_in_grams', 'calories', 'carbohydrates_in_grams', 'protein_in_grams']

    for col in numerical_cols:
        if col in df.columns and df[col].isnull().any():
            median_val = df[col].median()
            df[col].fillna(median_val, inplace=True)
            print(f"Filled missing values in '{col}' with median: {median_val}")

    # Handle missing categorical values
    categorical_cols = ['difficulty', 'category_name']
    for col in categorical_cols:
        if col in df.columns and df[col].isnull().any():
            mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'
            df[col].fillna(mode_val, inplace=True)
            print(f"Filled missing values in '{col}' with mode: {mode_val}")

    # Drop irrelevant columns
    columns_to_drop = [
        'prep_time_note', 'cook_time_note', 'image_attribution_name',
        'image_attribution_url', 'chef', 'source_url', 'category',
        'image_creative_commons', 'image'
    ]

    # Add ingredient and direction columns to drop list
    columns_to_drop.extend(ingredient_cols)
    columns_to_drop.extend(directions_cols)

    # Add measurement columns
    measurement_cols = [f'measurement_{i}' for i in range(1, 11) if f'measurement_{i}' in df.columns]
    columns_to_drop.extend(measurement_cols)

    # Only drop columns that exist
    columns_to_drop = [col for col in columns_to_drop if col in df.columns]
    df.drop(columns=columns_to_drop, inplace=True)

    print(f"Dropped {len(columns_to_drop)} irrelevant columns.")
    return df

# Preprocess the data
df = preprocess_data(df)

# Verify required columns exist
required_columns = ['id', 'recipe']
missing_columns = [col for col in required_columns if col not in df.columns]
if missing_columns:
    print(f"Error: Missing required columns: {missing_columns}")
    exit()

print("\nDataFrame after preprocessing:")
print(df.head(3))
print(f"Shape: {df.shape}")

# --- 3. Content-Based Filtering ---
def build_content_based_model(df):
    """Build content-based recommendation model"""

    # Define feature categories
    text_features = ['recipe', 'all_ingredients', 'all_directions']
    categorical_features = ['category_name', 'difficulty']
    numerical_features = [
        'prep_time_in_minutes', 'cook_time_in_minutes', 'serving',
        'calories', 'fat_in_grams', 'carbohydrates_in_grams', 'protein_in_grams'
    ]

    # Filter features that actually exist in the dataframe
    text_features = [col for col in text_features if col in df.columns]
    categorical_features = [col for col in categorical_features if col in df.columns]
    numerical_features = [col for col in numerical_features if col in df.columns]

    # Ensure text columns are strings
    for col in text_features:
        df[col] = df[col].astype(str).fillna('')

    # Build preprocessor
    transformers = []

    if text_features:
        for col in text_features:
            max_features = 1000 if col == 'recipe' else 2000
            transformers.append((f'{col}_text', TfidfVectorizer(stop_words='english', max_features=max_features), col))

    if categorical_features:
        transformers.append(('cat_enc', OneHotEncoder(handle_unknown='ignore'), categorical_features))

    if numerical_features:
        transformers.append(('num_scale', MinMaxScaler(), numerical_features))

    if not transformers:
        print("Error: No valid features found for preprocessing")
        return None, None

    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')

    print("Building feature matrix...")
    try:
        feature_matrix = preprocessor.fit_transform(df)
        print(f"Feature matrix shape: {feature_matrix.shape}")

        # Calculate cosine similarity
        print("Calculating cosine similarity...")
        cosine_sim = cosine_similarity(feature_matrix)
        print(f"Similarity matrix shape: {cosine_sim.shape}")

        return cosine_sim, preprocessor

    except Exception as e:
        print(f"Error building content-based model: {e}")
        return None, None

# Build content-based model
cosine_sim, preprocessor = build_content_based_model(df)

if cosine_sim is not None:
    # Create recipe ID to index mapping
    recipe_id_to_idx = pd.Series(df.index, index=df['id']).to_dict()

    def get_content_based_recommendations(recipe_id, num_recommendations=5):
        """Get content-based recommendations"""
        if recipe_id not in recipe_id_to_idx:
            print(f"Recipe ID {recipe_id} not found in the dataset.")
            return pd.DataFrame()

        idx = recipe_id_to_idx[recipe_id]
        sim_scores = list(enumerate(cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:num_recommendations+1]

        recipe_indices = [i[0] for i in sim_scores]
        similarity_scores = [i[1] for i in sim_scores]

        recommended_recipes = df.iloc[recipe_indices].copy()
        recommended_recipes['similarity_score'] = similarity_scores

        return recommended_recipes[['id', 'recipe', 'category_name', 'similarity_score']]

    # Test content-based recommendations
    test_recipe_id = df['id'].iloc[0]
    print(f"\n--- Content-Based Recommendations for Recipe ID: {test_recipe_id} ---")
    content_recommendations = get_content_based_recommendations(test_recipe_id, 5)
    print(content_recommendations)

# --- 4. Collaborative Filtering ---
def generate_synthetic_interactions(df, num_users=100):
    """Generate synthetic user interactions for demonstration"""
    np.random.seed(42)

    user_interactions = []
    recipe_ids = df['id'].unique()

    for user_id in range(1, num_users + 1):
        num_interactions = np.random.randint(5, 21)
        user_recipes = np.random.choice(recipe_ids, size=min(num_interactions, len(recipe_ids)), replace=False)

        for recipe_id in user_recipes:
            user_interactions.append({
                'user_id': user_id,
                'recipe_id': int(recipe_id),
                'interaction': 1
            })

    return pd.DataFrame(user_interactions)

def build_collaborative_model_fixed(df, interactions_df):
    """Build collaborative filtering model with proper index management"""

    # Create mappings - ensure we only use recipes that exist in both datasets
    unique_users = sorted(interactions_df['user_id'].unique())

    # Get recipes that exist in both interactions and the main dataframe
    interaction_recipes = set(interactions_df['recipe_id'].unique())
    df_recipes = set(df['id'].unique())
    valid_recipes = sorted(list(interaction_recipes.intersection(df_recipes)))

    print(f"Recipes in interactions: {len(interaction_recipes)}")
    print(f"Recipes in dataframe: {len(df_recipes)}")
    print(f"Valid recipes (intersection): {len(valid_recipes)}")

    if not valid_recipes:
        raise ValueError("No common recipes found between interactions and dataframe")

    # Filter interactions to only include valid recipes
    interactions_df = interactions_df[interactions_df['recipe_id'].isin(valid_recipes)].copy()

    user_id_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}
    recipe_id_to_idx = {recipe_id: idx for idx, recipe_id in enumerate(valid_recipes)}
    idx_to_recipe_id = {idx: recipe_id for recipe_id, idx in recipe_id_to_idx.items()}

    # Create user-item matrix
    interactions_df['user_idx'] = interactions_df['user_id'].map(user_id_to_idx)
    interactions_df['recipe_idx'] = interactions_df['recipe_id'].map(recipe_id_to_idx)

    # Remove any rows where mapping failed
    interactions_df = interactions_df.dropna(subset=['user_idx', 'recipe_idx'])

    data = interactions_df['interaction'].values
    row_ind = interactions_df['user_idx'].values.astype(int)
    col_ind = interactions_df['recipe_idx'].values.astype(int)

    user_item_matrix = csr_matrix(
        (data, (row_ind, col_ind)),
        shape=(len(unique_users), len(valid_recipes))
    )

    print(f"User-Item Matrix shape: {user_item_matrix.shape}")
    print(f"Sparsity: {1 - user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1]):.4f}")

    # Train ALS model
    item_user_matrix = user_item_matrix.T.tocsr()

    model = implicit.als.AlternatingLeastSquares(
        factors=50,
        regularization=0.01,
        iterations=20,
        random_state=42
    )

    print("Training collaborative filtering model...")
    model.fit(item_user_matrix)
    print("Model trained successfully!")

    return model, user_item_matrix, user_id_to_idx, idx_to_recipe_id

def get_collaborative_recommendations(user_id, num_recommendations=5):
    """Get collaborative filtering recommendations"""
    if user_id not in user_id_to_idx:
        print(f"User ID {user_id} not found in the interaction data.")
        return pd.DataFrame()

    user_idx = user_id_to_idx[user_id]

    try:
        # Get recommendations with filter_already_liked_items=False to avoid index issues
        recommendations = collab_model.recommend(
            user_idx,
            user_item_matrix[user_idx],
            N=num_recommendations,
            filter_already_liked_items=False
        )

        recommended_recipe_indices = [r[0] for r in recommendations]
        scores = [r[1] for r in recommendations]

        # Validate indices before mapping - this is the key fix
        valid_recommendations = []
        valid_scores = []

        for idx, score in zip(recommended_recipe_indices, scores):
            if idx in idx_to_recipe_id:
                valid_recommendations.append(idx_to_recipe_id[idx])
                valid_scores.append(score)

        if not valid_recommendations:
            print(f"No valid recommendations found for user {user_id}")
            return pd.DataFrame()

        # Get the recipes from the dataframe
        recommended_recipes = df[df['id'].isin(valid_recommendations)].copy()

        if recommended_recipes.empty:
            print(f"No matching recipes found in dataframe for user {user_id}")
            return pd.DataFrame()

        # Add scores in the correct order
        score_map = dict(zip(valid_recommendations, valid_scores))
        recommended_recipes['predicted_score'] = recommended_recipes['id'].map(score_map)

        return recommended_recipes[['id', 'recipe', 'category_name', 'predicted_score']].sort_values('predicted_score', ascending=False)

    except Exception as e:
        print(f"Error generating recommendations: {e}")
        return pd.DataFrame()

# Add this debugging section after building the collaborative model
print(f"Debug Info:")
print(f"user_item_matrix shape: {user_item_matrix.shape}")
print(f"Number of unique users in interactions: {len(user_id_to_idx)}")
print(f"Number of unique recipes in interactions: {len(idx_to_recipe_id)}")
print(f"Max recipe index in idx_to_recipe_id: {max(idx_to_recipe_id.keys()) if idx_to_recipe_id else 'None'}")
print(f"Min recipe index in idx_to_recipe_id: {min(idx_to_recipe_id.keys()) if idx_to_recipe_id else 'None'}")

print("\n" + "="*60)
print("TESTING RECOMMENDATION MODELS")
print("="*60)

# --- Test Content-Based Filtering ---
print("\n1. TESTING CONTENT-BASED FILTERING")
print("-" * 40)

if cosine_sim is not None:
    # Test with multiple recipes
    test_recipe_ids = df['id'].head(3).tolist()  # Test with first 3 recipes

    for recipe_id in test_recipe_ids:
        print(f"\nðŸ“ Content-Based Recommendations for Recipe ID: {recipe_id}")
        print(f"Original Recipe: {df[df['id'] == recipe_id]['recipe'].iloc[0]}")
        print(f"Category: {df[df['id'] == recipe_id]['category_name'].iloc[0] if 'category_name' in df.columns else 'N/A'}")

        recommendations = get_content_based_recommendations(recipe_id, 3)
        if not recommendations.empty:
            print("\nRecommended recipes:")
            for idx, row in recommendations.iterrows():
                print(f"  â€¢ {row['recipe']} (Score: {row['similarity_score']:.3f})")
        else:
            print("No recommendations found")
        print("-" * 50)

# --- Test Collaborative Filtering ---
print("\n2. TESTING COLLABORATIVE FILTERING")
print("-" * 40)

if 'collab_model' in locals():
    # Test with multiple users
    test_user_ids = list(user_id_to_idx.keys())[:3]  # Test with first 3 users

    for user_id in test_user_ids:
        print(f"\nðŸ‘¤ Collaborative Recommendations for User ID: {user_id}")

        # Show user's interaction history
        user_interactions = interactions_df[interactions_df['user_id'] == user_id]
        print(f"User has interacted with {len(user_interactions)} recipes")

        # Get sample of user's recipes
        sample_interactions = user_interactions.head(3)
        print("Sample of user's recipes:")
        for _, interaction in sample_interactions.iterrows():
            recipe_name = df[df['id'] == interaction['recipe_id']]['recipe'].iloc[0] if len(df[df['id'] == interaction['recipe_id']]) > 0 else "Unknown"
            print(f"  â€¢ {recipe_name}")

        recommendations = get_collaborative_recommendations(user_id, 3)
        if not recommendations.empty:
            print("\nRecommended recipes:")
            for idx, row in recommendations.iterrows():
                print(f"  â€¢ {row['recipe']} (Score: {row['predicted_score']:.3f})")
        else:
            print("No recommendations found")
        print("-" * 50)

# --- Test Model Performance Metrics ---
print("\n3. MODEL PERFORMANCE ANALYSIS")
print("-" * 40)

# Content-based model metrics
if cosine_sim is not None:
    print(f"âœ… Content-Based Model:")
    print(f"   - Similarity matrix shape: {cosine_sim.shape}")
    print(f"   - Coverage: {cosine_sim.shape[0]} recipes")
    print(f"   - Average similarity: {cosine_sim.mean():.4f}")

# Collaborative filtering metrics
if 'collab_model' in locals():
    print(f"âœ… Collaborative Filtering Model:")
    print(f"   - User-Item matrix shape: {user_item_matrix.shape}")
    print(f"   - Number of interactions: {user_item_matrix.nnz}")
    print(f"   - Sparsity: {(1 - user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1])):.4f}")
    print(f"   - Average interactions per user: {user_item_matrix.nnz / user_item_matrix.shape[0]:.1f}")

# --- Cross-Validation Test ---
print("\n4. CROSS-VALIDATION TEST")
print("-" * 40)

def test_recommendation_diversity():
    """Test how diverse the recommendations are"""
    if cosine_sim is not None:
        # Test content-based diversity
        test_recipe = df['id'].iloc[0]
        recommendations = get_content_based_recommendations(test_recipe, 10)

        if not recommendations.empty:
            unique_categories = recommendations['category_name'].nunique() if 'category_name' in recommendations.columns else 0
            print(f"Content-Based Diversity: {unique_categories} different categories in top 10 recommendations")

    if 'collab_model' in locals():
        # Test collaborative diversity
        test_user = list(user_id_to_idx.keys())[0]
        recommendations = get_collaborative_recommendations(test_user, 10)

        if not recommendations.empty:
            unique_categories = recommendations['category_name'].nunique() if 'category_name' in recommendations.columns else 0
            print(f"Collaborative Diversity: {unique_categories} different categories in top 10 recommendations")

test_recommendation_diversity()

# --- Save Test Results ---
print("\n5. SAVING TEST RESULTS")
print("-" * 40)

# Save a sample of recommendations for future reference
if cosine_sim is not None:
    test_recipe = df['id'].iloc[0]
    content_test_results = get_content_based_recommendations(test_recipe, 10)
    content_test_results.to_csv('/content/content_based_test_results.csv', index=False)
    print("âœ… Content-based test results saved to 'content_based_test_results.csv'")

if 'collab_model' in locals():
    test_user = list(user_id_to_idx.keys())[0]
    collab_test_results = get_collaborative_recommendations(test_user, 10)
    collab_test_results.to_csv('/content/collaborative_test_results.csv', index=False)
    print("âœ… Collaborative test results saved to 'collaborative_test_results.csv'")

print("\n" + "="*60)
print("TESTING COMPLETE!")
print("="*60)

import pickle
import joblib
from datetime import datetime
import os

def save_models_for_production():
    """Save models with proper versioning and metadata"""

    # Create models directory
    os.makedirs('models', exist_ok=True)

    # Version timestamp
    version = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Save content-based model
    content_model_data = {
        'cosine_sim': cosine_sim,
        'preprocessor': preprocessor,
        'recipe_id_to_idx': recipe_id_to_idx,
        'df_columns': df.columns.tolist(),
        'model_version': version,
        'model_type': 'content_based'
    }

    with open(f'/content/models/content_based_model_{version}.pkl', 'wb') as f:
        pickle.dump(content_model_data, f)

    # Save collaborative model
    collaborative_model_data = {
        'model': collab_model,
        'user_item_matrix': user_item_matrix,
        'user_id_to_idx': user_id_to_idx,
        'idx_to_recipe_id': idx_to_recipe_id,
        'model_version': version,
        'model_type': 'collaborative'
    }

    with open(f'/content/models/collaborative_model_{version}.pkl', 'wb') as f:
        pickle.dump(collaborative_model_data, f)

    # Save processed dataset
    df.to_csv(f'/content/models/processed_dataset_{version}.csv', index=False)

    # Save model metadata
    metadata = {
        'content_model_file': f'content_based_model_{version}.pkl',
        'collaborative_model_file': f'collaborative_model_{version}.pkl',
        'dataset_file': f'processed_dataset_{version}.csv',
        'created_at': datetime.now().isoformat(),
        'num_recipes': len(df),
        'num_users': len(user_id_to_idx),
        'matrix_sparsity': 1 - user_item_matrix.nnz / (user_item_matrix.shape[0] * user_item_matrix.shape[1])
    }

    with open(f'/content/models/model_metadata_{version}.json', 'w') as f:
        json.dump(metadata, f, indent=2)

    print(f"âœ… Models saved successfully with version: {version}")
    return version

# Save the models
model_version = save_models_for_production()